{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQwY4YRrHTuZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEnlUbgm8z3B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1krxJtKxpx",
        "outputId": "26505ea5-bbe3-44b4-cc2c-5e0c873597a5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgNSM8Xz79Mg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YqN1qY6lxTvt",
        "outputId": "26ab15fc-ee48-4681-a34e-ba41b72be7c0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-288976e7-b1fe-49e2-aea2-e1e191f6969b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>word_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mount Everest, towering at an awe-inspiring he...</td>\n",
              "      <td>B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Nestled in the majestic Himalayan range, strad...</td>\n",
              "      <td>O,O,O,O,B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling its formidable slopes is a monumental ...</td>\n",
              "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The summit of Mount Everest provides an unpara...</td>\n",
              "      <td>O,O,O,B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Rocky Mountains, often referred to as the ...</td>\n",
              "      <td>O,B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,O,O,O,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-288976e7-b1fe-49e2-aea2-e1e191f6969b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-288976e7-b1fe-49e2-aea2-e1e191f6969b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-288976e7-b1fe-49e2-aea2-e1e191f6969b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5fe2f1f3-d3ad-48c6-9d75-314784ffec8e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fe2f1f3-d3ad-48c6-9d75-314784ffec8e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5fe2f1f3-d3ad-48c6-9d75-314784ffec8e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                            sentence  \\\n",
              "0  Mount Everest, towering at an awe-inspiring he...   \n",
              "1  Nestled in the majestic Himalayan range, strad...   \n",
              "2  Scaling its formidable slopes is a monumental ...   \n",
              "3  The summit of Mount Everest provides an unpara...   \n",
              "4  The Rocky Mountains, often referred to as the ...   \n",
              "\n",
              "                                         word_labels  \n",
              "0  B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,O,O,O,O,...  \n",
              "1  O,O,O,O,B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,...  \n",
              "2            O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
              "3  O,O,O,B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,O,...  \n",
              "4  O,B-MOUNTAIN,I-MOUNTAIN,O,O,O,O,O,O,O,O,O,O,O,...  "
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label2id = {'O': 0, 'B-MOUNTAIN': 1,  'I-MOUNTAIN': 2}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "mountains_data = pd.read_csv(\"mountains_data.csv\")\n",
        "mountains_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XljBrtu6yVxu"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "    sentence = sentence.strip()\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "    return tokenized_sentence, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZmxKX0WyPEk"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]\n",
        "        word_labels = self.data.word_labels[index]\n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "\n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQj-Jo91xx0I",
        "outputId": "915034f5-6781-4064-c960-0fd62fcf151d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FULL Dataset: (49, 2)\n",
            "TRAIN Dataset: (39, 2)\n",
            "TEST Dataset: (10, 2)\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.8\n",
        "train_dataset = mountains_data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = mountains_data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(mountains_data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z4r41Hk8wE7"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWWFaun82BH",
        "outputId": "2071be2f-ca9f-4d01-9bf2-325730a2fe98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 256,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB6Riwmm89G7"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-woK_m7B9GQn"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tBqXoae9Gyv",
        "outputId": "99304f61-36d6-4045-b2f8-94dd0bb25298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 10 training steps: 1.0540688037872314\n",
            "Training loss epoch: 0.7365500062704087\n",
            "Training accuracy epoch: 0.7571449331260596\n",
            "Training epoch: 2\n",
            "Training loss per 10 training steps: 0.43886780738830566\n",
            "Training loss epoch: 0.23306655287742614\n",
            "Training accuracy epoch: 0.8514043582555735\n",
            "Training epoch: 3\n",
            "Training loss per 10 training steps: 0.3021300435066223\n",
            "Training loss epoch: 0.13877851516008377\n",
            "Training accuracy epoch: 0.8526514113793568\n",
            "Training epoch: 4\n",
            "Training loss per 10 training steps: 0.04740624129772186\n",
            "Training loss epoch: 0.10999319478869438\n",
            "Training accuracy epoch: 0.8551118380293008\n",
            "Training epoch: 5\n",
            "Training loss per 10 training steps: 0.07724394649267197\n",
            "Training loss epoch: 0.08589277863502502\n",
            "Training accuracy epoch: 0.8763957687915795\n",
            "Training epoch: 6\n",
            "Training loss per 10 training steps: 0.05567009374499321\n",
            "Training loss epoch: 0.07232280932366848\n",
            "Training accuracy epoch: 0.8828572722189664\n",
            "Training epoch: 7\n",
            "Training loss per 10 training steps: 0.0568513423204422\n",
            "Training loss epoch: 0.060138479620218274\n",
            "Training accuracy epoch: 0.9035307120088658\n",
            "Training epoch: 8\n",
            "Training loss per 10 training steps: 0.04820452630519867\n",
            "Training loss epoch: 0.05357470363378525\n",
            "Training accuracy epoch: 0.9154866271244118\n",
            "Training epoch: 9\n",
            "Training loss per 10 training steps: 0.04135477542877197\n",
            "Training loss epoch: 0.048910003155469894\n",
            "Training accuracy epoch: 0.9212287041750354\n",
            "Training epoch: 10\n",
            "Training loss per 10 training steps: 0.037181556224823\n",
            "Training loss epoch: 0.04602626170963049\n",
            "Training accuracy epoch: 0.9270605958577628\n",
            "Training epoch: 11\n",
            "Training loss per 10 training steps: 0.03493408113718033\n",
            "Training loss epoch: 0.04152514599263668\n",
            "Training accuracy epoch: 0.9416197718996724\n",
            "Training epoch: 12\n",
            "Training loss per 10 training steps: 0.033122602850198746\n",
            "Training loss epoch: 0.03760121800005436\n",
            "Training accuracy epoch: 0.9420356128227437\n",
            "Training epoch: 13\n",
            "Training loss per 10 training steps: 0.039694610983133316\n",
            "Training loss epoch: 0.03467463478446007\n",
            "Training accuracy epoch: 0.9496828313477893\n",
            "Training epoch: 14\n",
            "Training loss per 10 training steps: 0.03884843364357948\n",
            "Training loss epoch: 0.03240991216152907\n",
            "Training accuracy epoch: 0.9528761742565883\n",
            "Training epoch: 15\n",
            "Training loss per 10 training steps: 0.02127041108906269\n",
            "Training loss epoch: 0.030028223246335983\n",
            "Training accuracy epoch: 0.9517199177738629\n",
            "Training epoch: 16\n",
            "Training loss per 10 training steps: 0.037011340260505676\n",
            "Training loss epoch: 0.02852433007210493\n",
            "Training accuracy epoch: 0.9573265775146916\n",
            "Training epoch: 17\n",
            "Training loss per 10 training steps: 0.028934143483638763\n",
            "Training loss epoch: 0.025334217026829718\n",
            "Training accuracy epoch: 0.959165142662689\n",
            "Training epoch: 18\n",
            "Training loss per 10 training steps: 0.017561832442879677\n",
            "Training loss epoch: 0.023164790496230124\n",
            "Training accuracy epoch: 0.9668198971290151\n",
            "Training epoch: 19\n",
            "Training loss per 10 training steps: 0.04015105590224266\n",
            "Training loss epoch: 0.021556143835186958\n",
            "Training accuracy epoch: 0.9673401615435056\n",
            "Training epoch: 20\n",
            "Training loss per 10 training steps: 0.00964444875717163\n",
            "Training loss epoch: 0.019212585128843784\n",
            "Training accuracy epoch: 0.9760722906291885\n",
            "Training epoch: 21\n",
            "Training loss per 10 training steps: 0.009644542820751667\n",
            "Training loss epoch: 0.017872001603245737\n",
            "Training accuracy epoch: 0.9739532855594119\n",
            "Training epoch: 22\n",
            "Training loss per 10 training steps: 0.01391090452671051\n",
            "Training loss epoch: 0.016587769147008657\n",
            "Training accuracy epoch: 0.9794606781280526\n",
            "Training epoch: 23\n",
            "Training loss per 10 training steps: 0.012692246586084366\n",
            "Training loss epoch: 0.015074681537225843\n",
            "Training accuracy epoch: 0.980359929292692\n",
            "Training epoch: 24\n",
            "Training loss per 10 training steps: 0.015228357166051865\n",
            "Training loss epoch: 0.013664201740175486\n",
            "Training accuracy epoch: 0.9871472178341488\n",
            "Training epoch: 25\n",
            "Training loss per 10 training steps: 0.015755493193864822\n",
            "Training loss epoch: 0.012086063344031572\n",
            "Training accuracy epoch: 0.9908413290940148\n",
            "Training epoch: 26\n",
            "Training loss per 10 training steps: 0.008597033098340034\n",
            "Training loss epoch: 0.010549014387652277\n",
            "Training accuracy epoch: 0.9922914809491401\n",
            "Training epoch: 27\n",
            "Training loss per 10 training steps: 0.009592682123184204\n",
            "Training loss epoch: 0.009787422511726618\n",
            "Training accuracy epoch: 0.9946254272043745\n",
            "Training epoch: 28\n",
            "Training loss per 10 training steps: 0.008251617662608624\n",
            "Training loss epoch: 0.008082787413150072\n",
            "Training accuracy epoch: 0.994695792599568\n",
            "Training epoch: 29\n",
            "Training loss per 10 training steps: 0.0046694315969944\n",
            "Training loss epoch: 0.007806738279759884\n",
            "Training accuracy epoch: 0.9942496229899408\n",
            "Training epoch: 30\n",
            "Training loss per 10 training steps: 0.004483275581151247\n",
            "Training loss epoch: 0.006545874522998929\n",
            "Training accuracy epoch: 0.997374139270691\n",
            "Training epoch: 31\n",
            "Training loss per 10 training steps: 0.00667532766237855\n",
            "Training loss epoch: 0.005633773561567068\n",
            "Training accuracy epoch: 0.9991935483870968\n",
            "Training epoch: 32\n",
            "Training loss per 10 training steps: 0.004833464976400137\n",
            "Training loss epoch: 0.005440839764196426\n",
            "Training accuracy epoch: 0.9984051340704028\n",
            "Training epoch: 33\n",
            "Training loss per 10 training steps: 0.0027961423620581627\n",
            "Training loss epoch: 0.004741257289424539\n",
            "Training accuracy epoch: 0.9962540400775695\n",
            "Training epoch: 34\n",
            "Training loss per 10 training steps: 0.006391287315636873\n",
            "Training loss epoch: 0.0038835609215311707\n",
            "Training accuracy epoch: 0.999074074074074\n",
            "Training epoch: 35\n",
            "Training loss per 10 training steps: 0.003961819224059582\n",
            "Training loss epoch: 0.0041646708501502875\n",
            "Training accuracy epoch: 0.9974084055639502\n",
            "Training epoch: 36\n",
            "Training loss per 10 training steps: 0.0019396700663492084\n",
            "Training loss epoch: 0.003052205336280167\n",
            "Training accuracy epoch: 0.9992424242424243\n",
            "Training epoch: 37\n",
            "Training loss per 10 training steps: 0.0067940629087388515\n",
            "Training loss epoch: 0.002817211125511676\n",
            "Training accuracy epoch: 0.9991525423728813\n",
            "Training epoch: 38\n",
            "Training loss per 10 training steps: 0.0026069944724440575\n",
            "Training loss epoch: 0.002714219072367996\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 39\n",
            "Training loss per 10 training steps: 0.003370846388861537\n",
            "Training loss epoch: 0.0024646525853313506\n",
            "Training accuracy epoch: 0.9992647058823529\n",
            "Training epoch: 40\n",
            "Training loss per 10 training steps: 0.002378251403570175\n",
            "Training loss epoch: 0.0023246824741363524\n",
            "Training accuracy epoch: 0.9992307692307693\n",
            "Training epoch: 41\n",
            "Training loss per 10 training steps: 0.002478344365954399\n",
            "Training loss epoch: 0.0022691812948323785\n",
            "Training accuracy epoch: 0.9990990990990991\n",
            "Training epoch: 42\n",
            "Training loss per 10 training steps: 0.001525967032648623\n",
            "Training loss epoch: 0.0021358503843657674\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 43\n",
            "Training loss per 10 training steps: 0.0011679307790473104\n",
            "Training loss epoch: 0.0017862678098026663\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 44\n",
            "Training loss per 10 training steps: 0.00227216980420053\n",
            "Training loss epoch: 0.0015874822740443051\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 45\n",
            "Training loss per 10 training steps: 0.0016885162331163883\n",
            "Training loss epoch: 0.0017381547892000525\n",
            "Training accuracy epoch: 0.9991803278688526\n",
            "Training epoch: 46\n",
            "Training loss per 10 training steps: 0.0021875612437725067\n",
            "Training loss epoch: 0.0019463387783616782\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 47\n",
            "Training loss per 10 training steps: 0.003051119390875101\n",
            "Training loss epoch: 0.0013731778133660554\n",
            "Training accuracy epoch: 0.9992481203007518\n",
            "Training epoch: 48\n",
            "Training loss per 10 training steps: 0.0010275382082909346\n",
            "Training loss epoch: 0.0012070232129190116\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 49\n",
            "Training loss per 10 training steps: 0.0014425998087972403\n",
            "Training loss epoch: 0.0012091331940609963\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 50\n",
            "Training loss per 10 training steps: 0.0014499978860840201\n",
            "Training loss epoch: 0.0010768170468509197\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 51\n",
            "Training loss per 10 training steps: 0.000759782618843019\n",
            "Training loss epoch: 0.0010371447424404323\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 52\n",
            "Training loss per 10 training steps: 0.001002160832285881\n",
            "Training loss epoch: 0.001003363187192008\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 53\n",
            "Training loss per 10 training steps: 0.0012172305723652244\n",
            "Training loss epoch: 0.0009456262807361782\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 54\n",
            "Training loss per 10 training steps: 0.0005699676112271845\n",
            "Training loss epoch: 0.0009277616511099041\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 55\n",
            "Training loss per 10 training steps: 0.0005029513849876821\n",
            "Training loss epoch: 0.0007859996694605798\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 56\n",
            "Training loss per 10 training steps: 0.000698285992257297\n",
            "Training loss epoch: 0.0007938793249195441\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 57\n",
            "Training loss per 10 training steps: 0.0006900648004375398\n",
            "Training loss epoch: 0.0007449353754054755\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 58\n",
            "Training loss per 10 training steps: 0.0004892183351330459\n",
            "Training loss epoch: 0.0008918215549783781\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 59\n",
            "Training loss per 10 training steps: 0.0008418522775173187\n",
            "Training loss epoch: 0.0010405988345155493\n",
            "Training accuracy epoch: 0.9990990990990991\n",
            "Training epoch: 60\n",
            "Training loss per 10 training steps: 0.0006315857172012329\n",
            "Training loss epoch: 0.0007212426658952609\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 61\n",
            "Training loss per 10 training steps: 0.0013925947714596987\n",
            "Training loss epoch: 0.0007258515484863892\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 62\n",
            "Training loss per 10 training steps: 0.0006894272519275546\n",
            "Training loss epoch: 0.0006485708756372333\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 63\n",
            "Training loss per 10 training steps: 0.0009502554312348366\n",
            "Training loss epoch: 0.0006094119307817891\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 64\n",
            "Training loss per 10 training steps: 0.0011172422673553228\n",
            "Training loss epoch: 0.0005604789097560569\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 65\n",
            "Training loss per 10 training steps: 0.00037053521373309195\n",
            "Training loss epoch: 0.0005402043578214943\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 66\n",
            "Training loss per 10 training steps: 0.0002996013208758086\n",
            "Training loss epoch: 0.0005207968119066208\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 67\n",
            "Training loss per 10 training steps: 0.0003451355150900781\n",
            "Training loss epoch: 0.0005249975249171257\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 68\n",
            "Training loss per 10 training steps: 0.00025162947713397443\n",
            "Training loss epoch: 0.0005003975093131885\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 69\n",
            "Training loss per 10 training steps: 0.0003815608215518296\n",
            "Training loss epoch: 0.0004536635184194893\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 70\n",
            "Training loss per 10 training steps: 0.00034783093724399805\n",
            "Training loss epoch: 0.0004609983647242188\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 71\n",
            "Training loss per 10 training steps: 0.0003862195590045303\n",
            "Training loss epoch: 0.0004239182162564248\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 72\n",
            "Training loss per 10 training steps: 0.0002912728232331574\n",
            "Training loss epoch: 0.0004507385165197775\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 73\n",
            "Training loss per 10 training steps: 0.0006038655992597342\n",
            "Training loss epoch: 0.00042927115282509476\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 74\n",
            "Training loss per 10 training steps: 0.00035272238892503083\n",
            "Training loss epoch: 0.0004519785405136645\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 75\n",
            "Training loss per 10 training steps: 0.00035675428807735443\n",
            "Training loss epoch: 0.00042282590293325485\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 76\n",
            "Training loss per 10 training steps: 0.0002750827115960419\n",
            "Training loss epoch: 0.00041804320935625585\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 77\n",
            "Training loss per 10 training steps: 0.0005534814554266632\n",
            "Training loss epoch: 0.0003964927775086835\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 78\n",
            "Training loss per 10 training steps: 0.0005409637233242393\n",
            "Training loss epoch: 0.0003715666476637125\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 79\n",
            "Training loss per 10 training steps: 0.0001974186598090455\n",
            "Training loss epoch: 0.00033305007818853485\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 80\n",
            "Training loss per 10 training steps: 0.0009310557506978512\n",
            "Training loss epoch: 0.0004028494382509962\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 81\n",
            "Training loss per 10 training steps: 0.0004058846679981798\n",
            "Training loss epoch: 0.00034203677641926334\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 82\n",
            "Training loss per 10 training steps: 0.00026114771026186645\n",
            "Training loss epoch: 0.0004377972087240778\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 83\n",
            "Training loss per 10 training steps: 0.0003163788642268628\n",
            "Training loss epoch: 0.0005365703269490041\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 84\n",
            "Training loss per 10 training steps: 0.00038665684405714273\n",
            "Training loss epoch: 0.00040870252414606514\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 85\n",
            "Training loss per 10 training steps: 0.00022281605924945325\n",
            "Training loss epoch: 0.00048377851635450497\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 86\n",
            "Training loss per 10 training steps: 0.0002928856119979173\n",
            "Training loss epoch: 0.00042839566885959356\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 87\n",
            "Training loss per 10 training steps: 0.00023437214258592576\n",
            "Training loss epoch: 0.0003443494220846333\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 88\n",
            "Training loss per 10 training steps: 0.000506190990563482\n",
            "Training loss epoch: 0.00034138766350224616\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 89\n",
            "Training loss per 10 training steps: 0.00019999966025352478\n",
            "Training loss epoch: 0.00030393132619792594\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 90\n",
            "Training loss per 10 training steps: 0.00041549961315467954\n",
            "Training loss epoch: 0.00031047947268234564\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 91\n",
            "Training loss per 10 training steps: 0.0003180051571689546\n",
            "Training loss epoch: 0.0002745179706835188\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 92\n",
            "Training loss per 10 training steps: 0.0002055667864624411\n",
            "Training loss epoch: 0.00029553562053479253\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 93\n",
            "Training loss per 10 training steps: 0.00033080787397921085\n",
            "Training loss epoch: 0.00027332692843629047\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 94\n",
            "Training loss per 10 training steps: 0.00027737353229895234\n",
            "Training loss epoch: 0.0002710878979996778\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 95\n",
            "Training loss per 10 training steps: 0.00016009982209652662\n",
            "Training loss epoch: 0.0002586969523690641\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 96\n",
            "Training loss per 10 training steps: 0.000281939166598022\n",
            "Training loss epoch: 0.0002571194781921804\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 97\n",
            "Training loss per 10 training steps: 0.00020040120580233634\n",
            "Training loss epoch: 0.00027454943483462557\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 98\n",
            "Training loss per 10 training steps: 0.0003045911143999547\n",
            "Training loss epoch: 0.0002700772020034492\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 99\n",
            "Training loss per 10 training steps: 0.0001959070796146989\n",
            "Training loss epoch: 0.0002817342159687541\n",
            "Training accuracy epoch: 1.0\n",
            "Training epoch: 100\n",
            "Training loss per 10 training steps: 0.0002562973240856081\n",
            "Training loss epoch: 0.0002397361648036167\n",
            "Training accuracy epoch: 1.0\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvxTNAgr9Tj4"
      },
      "outputs": [],
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHFtLBs-9TPV",
        "outputId": "b9866856-d9ad-4a61-bd12-92cc436ab53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss per 100 evaluation steps: 0.011994196102023125\n",
            "Validation Loss: 0.02965855908114463\n",
            "Validation Accuracy: 0.9719989939005218\n"
          ]
        }
      ],
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mf2X9VxIFJa",
        "outputId": "3971e543-2374-42d8-c8ef-76da49d2694c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    MOUNTAIN       0.73      0.86      0.79        22\n",
            "\n",
            "   micro avg       0.73      0.86      0.79        22\n",
            "   macro avg       0.73      0.86      0.79        22\n",
            "weighted avg       0.73      0.86      0.79        22\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "Pb-a3T5koPcg"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'weights/bert-ner-mountains.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
